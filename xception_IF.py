# -*- coding: utf-8 -*-
"""Xception mit hyperportOptimazionmit1Ä±nput .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OOSzWTGWQJTb6CF2j1Ofr7X0FC6YcBTR
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import datetime
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Lambda, GlobalAveragePooling2D, Dense, BatchNormalization, Dropout
from tensorflow.keras.applications import Xception
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard
from tensorflow.keras.regularizers import l1_l2
from hyperopt import hp, fmin, tpe, Trials, STATUS_OK
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Check GPU availability
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

# Define paths
train_images_path1 = '/content/drive/My Drive/DATASETS3/train_images1tif'
train_images_path2 = '/content/drive/My Drive/DATASETS3/train_images2tif'
val_images_path = '/content/drive/My Drive/DATASETS3/val_imagestif'
test_images_path = '/content/drive/My Drive/DATASETS3/testimagestif'

# Load CSV data
train_data1 = pd.read_csv('/content/drive/My Drive/DATASETS3/normalized_trainimages1.csv', sep=';')
train_data2 = pd.read_csv('/content/drive/My Drive/DATASETS3/normalized_trainimages2.csv', sep=';')
val_data = pd.read_csv('/content/drive/My Drive/DATASETS3/normalized_valimages.csv', sep=';')
test_data = pd.read_csv('/content/drive/My Drive/DATASETS3/normalized_testimages.csv', sep=';')

# Correct image paths if necessary
train_data1['Image Name '] = train_data1['Image Name '].apply(lambda x: os.path.join(train_images_path1, f"{x}.tif"))
train_data2['Image Name '] = train_data2['Image Name '].apply(lambda x: os.path.join(train_images_path2, f"{x}.tif"))
val_data['Image Name '] = val_data['Image Name '].apply(lambda x: os.path.join(val_images_path, f"{x}.tif"))
test_data['Image Name '] = test_data['Image Name '].apply(lambda x: os.path.join(test_images_path, f"{x}.tif"))

# Combine training data
combined_train_data = pd.concat([train_data1, train_data2])

# Function to calculate IQR bounds
def calculate_iqr_bounds(df, columns):
    bounds = {}
    for column in columns:
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        bounds[column] = {
            'lower_bound': Q1 - 1.5 * IQR,
            'upper_bound': Q3 + 1.5 * IQR
        }
    return bounds

# Function to remove outliers using IQR bounds
def remove_outliers_using_bounds(df, bounds):
    for column in bounds:
        df = df[(df[column] >= bounds[column]['lower_bound']) & (df[column] <= bounds[column]['upper_bound'])]
    return df

# Define numerical columns to check for outliers
numerical_columns = ["Cell Radius","Area","Migration Radius","Distrubition of Migration","Number of Projections","Compactness"]

# Calculate IQR bounds
iqr_bounds = calculate_iqr_bounds(combined_train_data, numerical_columns)

# Remove outliers from training and validation data
combined_train_data_cleaned = remove_outliers_using_bounds(combined_train_data, iqr_bounds)
val_data_cleaned = remove_outliers_using_bounds(val_data, iqr_bounds)

# Create image data generators
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Augment training data
train_datagen_augmented = ImageDataGenerator(
    rescale=1./255,
    rotation_range=45,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Create data generators
train_generator = train_datagen_augmented.flow_from_dataframe(
    dataframe=combined_train_data_cleaned,
    x_col='Image Name ',
    y_col=["Cell Radius", "Area", "Migration Radius", "Distrubition of Migration", "Number of Projections", "Compactness"],
    color_mode='grayscale',
    class_mode='raw',
    target_size=(299, 299),
    batch_size=18
)

val_generator = val_datagen.flow_from_dataframe(
    dataframe=val_data_cleaned,
    x_col='Image Name ',
    y_col=["Cell Radius", "Area", "Migration Radius", "Distrubition of Migration", "Number of Projections", "Compactness"],
    color_mode='grayscale',
    class_mode='raw',
    target_size=(299, 299),
    batch_size=18
)

test_generator = test_datagen.flow_from_dataframe(
    dataframe=test_data,
    x_col='Image Name ',
    y_col=["Cell Radius", "Area", "Migration Radius", "Distrubition of Migration", "Number of Projections", "Compactness"],
    color_mode='grayscale',
    class_mode='raw',
    target_size=(299, 299),
    batch_size=18
)

# Function to expand image dimensions from 1 to 3 channels
def expand_dims(input):
    return tf.tile(input, [1, 1, 1, 3])

# Define objective function for hyperparameter optimization
def objective(params):
    # Clear previous model to free memory
    tf.keras.backend.clear_session()

    # Define the input layer and expand dimensions within the function
    input_tensor = Input(shape=(299, 299, 1))
    x = Lambda(lambda x: tf.tile(x, [1, 1, 1, 3]))(input_tensor)

    # Load base model with adjusted input layer
    base_model = Xception(weights='imagenet', include_top=False, input_tensor=x)

    # Add custom layers on top of the base model
    x = base_model.output
    x = GlobalAveragePooling2D()(x)

    for i in range(params['num_layers']):
        x = BatchNormalization()(x)
        x = Dense(params[f'units_{i}'], activation=params['activation'], kernel_regularizer=l1_l2(l1=params['l1'], l2=params['l2']))(x)
        if params['dropout']:
            x = Dropout(params['dropout_rate'])(x)

    predictions = Dense(6, activation='linear')(x)

    model = Model(inputs=input_tensor, outputs=predictions)

    model.compile(optimizer=Adam(learning_rate=params['lr']),
                  loss='mean_squared_error',
                  metrics=['mean_squared_error', 'mean_absolute_error'])

    history = model.fit(train_generator, epochs=params['epochs'],
                        validation_data=val_generator, verbose=0)

    val_loss = np.min(history.history['val_loss'])
    return {'loss': val_loss, 'status': STATUS_OK}

# Define hyperparameter search space
search_space = {
    'lr': hp.loguniform('lr', np.log(0.0001), np.log(0.01)),
    'epochs': hp.choice('epochs', [10, 40, 70, 100, 130]),
    'num_layers': hp.choice('num_layers', [1, 2, 3, 4, 5, 6, 7]),
    'units_0': hp.choice('units_0', [128, 256, 512, 1024, 2048]),
    'units_1': hp.choice('units_1', [128, 256, 512, 1024, 2048]),
    'units_2': hp.choice('units_2', [128, 256, 512, 1024, 2048]),
    'units_3': hp.choice('units_3', [128, 256, 512, 1024, 2048]),
    'units_4': hp.choice('units_4', [128, 256, 512, 1024, 2048]),
    'units_5': hp.choice('units_5', [128, 256, 512, 1024, 2048]),
    'units_6': hp.choice('units_6', [128, 256, 512, 1024, 2048]),

    'activation': hp.choice('activation', ['relu', 'tanh', 'sigmoid']),
    'l1': hp.loguniform('l1', np.log(1e-6), np.log(1e-2)),
    'l2': hp.loguniform('l2', np.log(1e-6), np.log(1e-2)),
    'dropout': hp.choice('dropout', [True, False]),
    'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.5)
}

# Perform hyperparameter optimization
trials = Trials()
best = fmin(fn=objective, space=search_space, algo=tpe.suggest, max_evals=90, trials=trials)
print("Best: ", best)

# Save best hyperparameters
best_params = {
    'learning_rate': best['lr'],
    'epochs': [10, 40, 70, 100, 130][best['epochs']],
    'num_layers': [1, 2, 3, 4, 5, 6, 7][best['num_layers']],
    'units_0': [128, 256, 512, 1024, 2048][best['units_0']],
    'units_1': [128, 256, 512, 1024, 2048][best['units_1']],
    'units_2': [128, 256, 512, 1024, 2048][best['units_2']],
    'units_3': [128, 256, 512, 1024, 2048][best['units_3']],
    'units_4': [128, 256, 512, 1024, 2048][best['units_4']],
    'units_5': [128, 256, 512, 1024, 2048][best['units_5']],
    'units_6': [128, 256, 512, 1024, 2048][best['units_5']],
    'activation': ['relu', 'tanh', 'sigmoid'][best['activation']],
    'l1': best['l1'],
    'l2': best['l2'],
    'dropout': [True, False][best['dropout']],
    'dropout_rate': best['dropout_rate']
}

hyperparameters_df = pd.DataFrame(list(best_params.items()), columns=['Hyperparameter', 'Wert'])
csv_save_path = '/content/drive/My Drive/CNNMODEL/HyperparameterXception5.csv'
hyperparameters_df.to_csv(csv_save_path, index=False)
print(f"Hyperparameter gespeichert unter: {csv_save_path}")

# TensorBoard log directory
log_dir = os.path.join("logs", "fitxception", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

# ReduceLROnPlateau callback configuration
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)

# Compile and train the model with best hyperparameters
input_tensor = Input(shape=(299, 299, 1))
x = Lambda(lambda x: tf.tile(x, [1, 1, 1, 3]))(input_tensor)
base_model = Xception(weights='imagenet', include_top=False, input_tensor=x)
x = base_model.output
x = GlobalAveragePooling2D()(x)

for i in range(best_params['num_layers']):
    x = BatchNormalization()(x)
    x = Dense(best_params[f'units_{i}'], activation=best_params['activation'], kernel_regularizer=l1_l2(l1=best_params['l1'], l2=best_params['l2']))(x)
    if best_params['dropout']:
        x = Dropout(best_params['dropout_rate'])(x)

predictions = Dense(6, activation='linear')(x)
model = Model(inputs=input_tensor, outputs=predictions)

model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']),
              loss='mean_squared_error',
              metrics=['mean_squared_error', 'mean_absolute_error'])

history = model.fit(train_generator, epochs=best_params['epochs'], validation_data=val_generator,
                    callbacks=[tensorboard_callback, reduce_lr, EarlyStopping(patience=10, min_delta=0.001, restore_best_weights=True)])

# Save training metrics
results = {
    'Epoch': range(1, len(history.history['loss']) + 1),
    'Training Loss': history.history['loss'],
    'Validation Loss': history.history['val_loss'],
    'Training MSE': history.history['mean_squared_error'],
    'Validation MSE': history.history['val_mean_squared_error'],
    'Training MAE': history.history['mean_absolute_error'],
    'Validation MAE': history.history['val_mean_absolute_error']
}
results_df = pd.DataFrame(results)
results_df.to_csv('/content/drive/My Drive/CNNMODEL/Training_Metricsmit1InputXception5.csv', index=False)

# Evaluate the model on test data
test_steps_per_epoch = np.ceil(test_generator.samples / test_generator.batch_size)
test_loss, test_mse, test_mae = model.evaluate(test_generator, steps=test_steps_per_epoch)

# Save test metrics
test_results = {
    "Metrik": ["Loss", "MSE", "MAE"],
    "Testwert": [test_loss, test_mse, test_mae]
}
test_results_df = pd.DataFrame(test_results)
test_results_df.to_csv('/content/drive/My Drive/CNNMODEL/Test_Metrics_mit_1_InputXception5.csv', index=False)

# Save the model
model_save_path = '/content/drive/My Drive/CNNMODEL/Mein_Trainiertes_Modellmit1Input-Xception5'
model.save(model_save_path)
print(f"Modell gespeichert unter: {model_save_path}")

# Plot training and validation MSE
plt.figure(figsize=(12, 6))
plt.plot(history.history['mean_squared_error'], label='Training MSE')
plt.plot(history.history['val_mean_squared_error'], label='Validation MSE')
plt.title('Training and Validation MSE')
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.savefig('/content/drive/My Drive/CNNMODEL/MSE_PlotXception5.pdf')
plt.show()

# Plot training and validation MAE
plt.figure(figsize=(12, 6))
plt.plot(history.history['mean_absolute_error'], label='Training MAE')
plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')
plt.title('Training and Validation MAE')
plt.xlabel('Epoch')
plt.ylabel('Mean Absolute Error')
plt.legend()
plt.savefig('/content/drive/My Drive/CNNMODEL/MAE_PlotXception5.pdf')
plt.show()