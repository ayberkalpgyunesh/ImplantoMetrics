# -*- coding: utf-8 -*-
"""EndSHAPWerte berechnung.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12dtQrCEV6WpUVtN1byh1g1rXJ5FjJgEk
"""

# Bibliotheken importieren
!pip install scikit-optimize
!pip install joblib
!pip install umap-learn
!pip install tensorflow
!pip install xgboost

!pip install shap


import numpy as np
import shap
import xgboost as xgb
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from skopt import BayesSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf
from math import sqrt
import joblib
from google.colab import drive
from scipy import stats
from scipy.stats import ttest_ind
import matplotlib.pyplot as plt



# Funktion zum Entfernen von Ausreißern
def remove_outliers(features, labels, threshold=3):
    z_scores = np.abs(stats.zscore(features))
    valid_rows = (z_scores < threshold).all(axis=1)
    return features[valid_rows], labels[valid_rows]

# Google Drive mounten und Modell laden
drive.mount('/content/drive', force_remount=True)
model_path = '/content/drive/My Drive/CNNMODEL/alpXception-14'
model = load_model(model_path)

# Feature-Extractor erstellen
extractor = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-2].output)

# Funktion zur Datenladung und Feature-Extraktion
def load_and_extract_features(path_to_data, csv_file, images_folder, batch_size=18, selected_features=None):
    data = pd.read_csv(path_to_data + csv_file, sep=';')
    data['Image Name '] = data['Image Name '].apply(lambda x: f'{x}.tif')
    datagen = ImageDataGenerator(rescale=1./255)
    generator = datagen.flow_from_dataframe(
        dataframe=data,
        directory=path_to_data + images_folder,
        x_col="Image Name ",
        y_col=["Time", "Cell Radius", "Area", "Migration Radius", "Distrubition of Migration", "Number of Projections", "Compactness"],
        target_size=(299, 299),
        batch_size=batch_size,
        class_mode='raw'
    )

    features = []
    labels = []
    for _ in range(len(generator)):
        x, y = generator.next()
        extracted_features = extractor.predict([x, x])
        if selected_features is not None:
            extracted_features = extracted_features[:, selected_features]
        features.append(extracted_features)
        labels.append(y)

    return np.concatenate(features, axis=0), np.concatenate(labels, axis=0)

# Pfad zu den Trainings- und Validierungsdaten festlegen
path_to_data = '/content/drive/My Drive/DATASETS3/'
selected_features_indices = [1, 97, 104, 154, 157, 163, 173, 223, 234, 252, 257, 276, 294, 315, 355, 358, 359, 367, 383, 392, 410, 506]

# Laden und Extrahieren von Features für zwei Trainingsdatensätze
train_features_1, train_labels_1 = load_and_extract_features(path_to_data, 'normalized_trainimages1.csv', 'train_images1tif', selected_features=selected_features_indices)
train_features_2, train_labels_2 = load_and_extract_features(path_to_data, 'normalized_trainimages2.csv', 'train_images2tif', selected_features=selected_features_indices)

# Kombinieren der Features und Labels beider Trainingsdatensätze
combined_features = np.concatenate([train_features_1, train_features_2], axis=0)
combined_labels = np.concatenate([train_labels_1, train_labels_2], axis=0)

# Laden und Extrahieren von Features für Validierungsdatensatz
val_features, val_labels = load_and_extract_features(path_to_data, 'normalized_valimages.csv', 'val_imagestif', selected_features=selected_features_indices)

# Ausreißer entfernen für Trainingsdatensätze
train_features_1, train_labels_1 = remove_outliers(train_features_1, train_labels_1)
train_features_2, train_labels_2 = remove_outliers(train_features_2, train_labels_2)

# Kombinieren der Features und Labels beider Trainingsdatensätze
combined_features = np.concatenate([train_features_1, train_features_2], axis=0)
combined_labels = np.concatenate([train_labels_1, train_labels_2], axis=0)

# Ausreißer entfernen für Validierungsdatensatz
val_features, val_labels = remove_outliers(val_features, val_labels)


# Normalisieren der 'Time'-Variable
scaler_time = MinMaxScaler()
combined_labels_normalized = scaler_time.fit_transform(combined_labels[:, 0].reshape(-1, 1)).flatten()
val_labels_normalized = scaler_time.transform(val_labels[:, 0].reshape(-1, 1)).flatten()

# Trainings- und Validierungsdaten vorbereiten
X_train_params = combined_labels[:, 1:]  # Gemessene Parameter
X_train_cnn = combined_features  # CNN-Features
y_train = combined_labels_normalized  # Normalisierte Zeit als Zielvariable

X_val_params = val_labels[:, 1:]  # Gemessene Parameter für Validierung
X_val_cnn = val_features  # CNN-Features für Validierung
y_val = val_labels_normalized  # Normalisierte Zeit als Zielvariable für Validierung

# Hyperparameter-Suchraum definieren
search_space = {
    'n_estimators': (100, 1000),
    'learning_rate': (0.01, 0.1),
    'max_depth': (3, 10),
    'subsample': (0.5, 0.9),
    'colsample_bytree': (0.5, 0.9)
}

# BayesSearchCV für Modell A (gemessene Parameter)
opt_A = BayesSearchCV(
    xgb.XGBRegressor(objective='reg:squarederror', random_state=42),
    search_space,
    n_iter=32,
    cv=3,
    n_jobs=-1,
    random_state=0
)
opt_A.fit(X_train_params, y_train)

# BayesSearchCV für Modell B (CNN-Features)
opt_B = BayesSearchCV(
    xgb.XGBRegressor(objective='reg:squarederror', random_state=42),
    search_space,
    n_iter=32,
    cv=3,
    n_jobs=-1,
    random_state=0
)
opt_B.fit(X_train_cnn, y_train)

# Beste Modelle für Modell A und Modell B anzeigen
print("Beste Parameter für Modell A: ", opt_A.best_params_)
print("Beste Parameter für Modell B: ", opt_B.best_params_)

# Modell A (gemessene Parameter) mit den besten Parametern trainieren und auf Validierungsdaten evaluieren
model_A = opt_A.best_estimator_
y_pred_val_A = model_A.predict(X_val_params)
mse_A = mean_squared_error(y_val, y_pred_val_A)
mae_A = mean_absolute_error(y_val, y_pred_val_A)
rmse_A = sqrt(mse_A)
print("MSE (Model A - Validation): ", mse_A)
print("MAE (Model A - Validation): ", mae_A)
print("RMSE (Model A - Validation): ", rmse_A)

# Modell B (CNN-Features) mit den besten Parametern trainieren und auf Validierungsdaten evaluieren
model_B = opt_B.best_estimator_
y_pred_val_B = model_B.predict(X_val_cnn)
mse_B = mean_squared_error(y_val, y_pred_val_B)
mae_B = mean_absolute_error(y_val, y_pred_val_B)
rmse_B = sqrt(mse_B)
print("MSE (Model B - Validation): ", mse_B)
print("MAE (Model B - Validation): ", mae_B)
print("RMSE (Model B - Validation): ", rmse_B)

# Gewichtung der Vorhersagen (kann je nach Leistung angepasst werden)
weight_A = 0.5  # Gewicht für Modell A
weight_B = 0.5  # Gewicht für Modell B

# Berechnung der Gewichtungen basierend auf den MSE-Werten
weight_A = 1 / mse_A
weight_B = 1 / mse_B
total_weight = weight_A + weight_B
weight_A_normalized = weight_A / total_weight
weight_B_normalized = weight_B / total_weight

# Kombination der Vorhersagen auf Validierungsdaten durch gewichteten Durchschnitt
y_pred_combined = (weight_A_normalized * y_pred_val_A) + (weight_B_normalized * y_pred_val_B)

# Modellbewertung für das kombinierte Modell auf Validierungsdaten
mse_combined = mean_squared_error(y_val, y_pred_combined)
mae_combined = mean_absolute_error(y_val, y_pred_combined)
rmse_combined = sqrt(mse_combined)
print("Gewicht für Modell A: ", weight_A_normalized)
print("Gewicht für Modell B: ", weight_B_normalized)
print("MSE (Combined Model - Validation): ", mse_combined)
print("MAE (Combined Model - Validation): ", mae_combined)
print("RMSE (Combined Model - Validation): ", rmse_combined)

# Bibliotheken importieren
!pip install scikit-optimize
!pip install joblib
!pip install umap-learn
!pip install tensorflow
!pip install xgboost

!pip install shap


import numpy as np
import shap
import xgboost as xgb
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from skopt import BayesSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf
from math import sqrt
import joblib
from google.colab import drive
from scipy import stats
from scipy.stats import ttest_ind
import matplotlib.pyplot as plt



# Funktion zum Entfernen von Ausreißern
def remove_outliers(features, labels, threshold=3):
    z_scores = np.abs(stats.zscore(features))
    valid_rows = (z_scores < threshold).all(axis=1)
    return features[valid_rows], labels[valid_rows]

# Google Drive mounten und Modell laden
drive.mount('/content/drive', force_remount=True)
model_path = '/content/drive/My Drive/CNNMODEL/alpXception-14'
model = load_model(model_path)

# Feature-Extractor erstellen
extractor = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-2].output)

# Funktion zur Datenladung und Feature-Extraktion
def load_and_extract_features(path_to_data, csv_file, images_folder, batch_size=18, selected_features=None):
    data = pd.read_csv(path_to_data + csv_file, sep=';')
    data['Image Name '] = data['Image Name '].apply(lambda x: f'{x}.tif')
    datagen = ImageDataGenerator(rescale=1./255)
    generator = datagen.flow_from_dataframe(
        dataframe=data,
        directory=path_to_data + images_folder,
        x_col="Image Name ",
        y_col=["Time", "Cell Radius", "Area", "Migration Radius", "Distrubition of Migration", "Number of Projections", "Compactness"],
        target_size=(299, 299),
        batch_size=batch_size,
        class_mode='raw'
    )

    features = []
    labels = []
    for _ in range(len(generator)):
        x, y = generator.next()
        extracted_features = extractor.predict([x, x])
        if selected_features is not None:
            extracted_features = extracted_features[:, selected_features]
        features.append(extracted_features)
        labels.append(y)

    return np.concatenate(features, axis=0), np.concatenate(labels, axis=0)

# Pfad zu den Trainings- und Validierungsdaten festlegen
path_to_data = '/content/drive/My Drive/DATASETS3/'
selected_features_indices = [1, 97, 104, 154, 157, 163, 173, 223, 234, 252, 257, 276, 294, 315, 355, 358, 359, 367, 383, 392, 410, 506]

# Laden und Extrahieren von Features für zwei Trainingsdatensätze
train_features_1, train_labels_1 = load_and_extract_features(path_to_data, 'normalized_trainimages1.csv', 'train_images1tif', selected_features=selected_features_indices)
train_features_2, train_labels_2 = load_and_extract_features(path_to_data, 'normalized_trainimages2.csv', 'train_images2tif', selected_features=selected_features_indices)
# Laden und Extrahieren von Features für den dritten Trainingsdatensatz
train_features_3, train_labels_3 = load_and_extract_features(path_to_data, 'normalized_testimages.csv', 'test_imagestif', selected_features=selected_features_indices)

# Kombinieren der Features und Labels aller drei Trainingsdatensätze
combined_features = np.concatenate([train_features_1, train_features_2, train_features_3], axis=0)
combined_labels = np.concatenate([train_labels_1, train_labels_2, train_labels_3], axis=0)


# Laden und Extrahieren von Features für Validierungsdatensatz
val_features, val_labels = load_and_extract_features(path_to_data, 'normalized_valimages.csv', 'val_imagestif', selected_features=selected_features_indices)

# Ausreißer entfernen für Trainingsdatensätze
train_features_1, train_labels_1 = remove_outliers(train_features_1, train_labels_1)
train_features_2, train_labels_2 = remove_outliers(train_features_2, train_labels_2)

# Kombinieren der Features und Labels beider Trainingsdatensätze
combined_features = np.concatenate([train_features_1, train_features_2], axis=0)
combined_labels = np.concatenate([train_labels_1, train_labels_2], axis=0)

# Ausreißer entfernen für Validierungsdatensatz
val_features, val_labels = remove_outliers(val_features, val_labels)


# Normalisieren der 'Time'-Variable
scaler_time = MinMaxScaler()
combined_labels_normalized = scaler_time.fit_transform(combined_labels[:, 0].reshape(-1, 1)).flatten()
val_labels_normalized = scaler_time.transform(val_labels[:, 0].reshape(-1, 1)).flatten()

# Trainings- und Validierungsdaten vorbereiten
X_train_params = combined_labels[:, 1:]  # Gemessene Parameter
X_train_cnn = combined_features  # CNN-Features
y_train = combined_labels_normalized  # Normalisierte Zeit als Zielvariable

X_val_params = val_labels[:, 1:]  # Gemessene Parameter für Validierung
X_val_cnn = val_features  # CNN-Features für Validierung
y_val = val_labels_normalized  # Normalisierte Zeit als Zielvariable für Validierung

# Hyperparameter-Suchraum definieren
search_space = {
    'n_estimators': (100, 1000),
    'learning_rate': (0.01, 0.1),
    'max_depth': (3, 10),
    'subsample': (0.5, 0.9),
    'colsample_bytree': (0.5, 0.9)
}

# BayesSearchCV für Modell A (gemessene Parameter)
opt_A = BayesSearchCV(
    xgb.XGBRegressor(objective='reg:squarederror', random_state=42),
    search_space,
    n_iter=32,
    cv=3,
    n_jobs=-1,
    random_state=0
)
opt_A.fit(X_train_params, y_train)

# BayesSearchCV für Modell B (CNN-Features)
opt_B = BayesSearchCV(
    xgb.XGBRegressor(objective='reg:squarederror', random_state=42),
    search_space,
    n_iter=32,
    cv=3,
    n_jobs=-1,
    random_state=0
)
opt_B.fit(X_train_cnn, y_train)

# Beste Modelle für Modell A und Modell B anzeigen
print("Beste Parameter für Modell A: ", opt_A.best_params_)
print("Beste Parameter für Modell B: ", opt_B.best_params_)

# Modell A (gemessene Parameter) mit den besten Parametern trainieren und auf Validierungsdaten evaluieren
model_A = opt_A.best_estimator_
y_pred_val_A = model_A.predict(X_val_params)
mse_A = mean_squared_error(y_val, y_pred_val_A)
mae_A = mean_absolute_error(y_val, y_pred_val_A)
rmse_A = sqrt(mse_A)
print("MSE (Model A - Validation): ", mse_A)
print("MAE (Model A - Validation): ", mae_A)
print("RMSE (Model A - Validation): ", rmse_A)

# Modell B (CNN-Features) mit den besten Parametern trainieren und auf Validierungsdaten evaluieren
model_B = opt_B.best_estimator_
y_pred_val_B = model_B.predict(X_val_cnn)
mse_B = mean_squared_error(y_val, y_pred_val_B)
mae_B = mean_absolute_error(y_val, y_pred_val_B)
rmse_B = sqrt(mse_B)
print("MSE (Model B - Validation): ", mse_B)
print("MAE (Model B - Validation): ", mae_B)
print("RMSE (Model B - Validation): ", rmse_B)

# Gewichtung der Vorhersagen (kann je nach Leistung angepasst werden)
weight_A = 0.5  # Gewicht für Modell A
weight_B = 0.5  # Gewicht für Modell B

# Berechnung der Gewichtungen basierend auf den MSE-Werten
weight_A = 1 / mse_A
weight_B = 1 / mse_B
total_weight = weight_A + weight_B
weight_A_normalized = weight_A / total_weight
weight_B_normalized = weight_B / total_weight

# Kombination der Vorhersagen auf Validierungsdaten durch gewichteten Durchschnitt
y_pred_combined = (weight_A_normalized * y_pred_val_A) + (weight_B_normalized * y_pred_val_B)

# Modellbewertung für das kombinierte Modell auf Validierungsdaten
mse_combined = mean_squared_error(y_val, y_pred_combined)
mae_combined = mean_absolute_error(y_val, y_pred_combined)
rmse_combined = sqrt(mse_combined)
print("Gewicht für Modell A: ", weight_A_normalized)
print("Gewicht für Modell B: ", weight_B_normalized)
print("MSE (Combined Model - Validation): ", mse_combined)
print("MAE (Combined Model - Validation): ", mae_combined)
print("RMSE (Combined Model - Validation): ", rmse_combined)

!pip install shap
!pip install tqdm  # Installieren Sie tqdm, falls noch nicht geschehen
import numpy as np
import shap
from tqdm import tqdm  # Importieren Sie tqdm
from xgboost import XGBRegressor
from tqdm.auto import tqdm  # Verwenden Sie auto, damit tqdm korrekt in Jupyter Notebooks funktioniert
import pandas as pd


# Annahme: X_train_params, X_train_cnn, combined_labels, y_train sind bereits definiert

# Interaktionsfeatures zwischen Parametern und CNN-Features generieren
interaction_params_cnn = np.array([
    X_train_params[:, i].reshape(-1, 1) * X_train_cnn[:, j]
    for i in range(X_train_params.shape[1])
    for j in range(X_train_cnn.shape[1])
]).reshape(X_train_params.shape[0], -1)

# Kombinieren der ursprünglichen Features und der Interaktionsfeatures
X_train_combined = np.concatenate([X_train_params, X_train_cnn, interaction_params_cnn], axis=1)

# Trainieren des kombinierten Modells und Berechnen der SHAP-Werte
model_B = XGBRegressor()  # oder Ihr vorher trainiertes Modell
model_B.fit(X_train_combined, y_train)
explainer_B = shap.TreeExplainer(model_B)
shap_values_B = explainer_B.shap_values(X_train_combined)

# SHAP-Explainer für Modell A (nur gemessene Parameter)
model_A = XGBRegressor()  # oder Ihr vorher trainiertes Modell
model_A.fit(X_train_params, y_train)
explainer_A = shap.Explainer(model_A)
shap_values_A = explainer_A.shap_values(X_train_params)

# Aggregieren der SHAP-Werte über Zeitintervalle
aggregated_shap_values_list_A = []
aggregated_shap_values_list_B = []
max_time = 144  # Zum Beispiel 144 Stunden insgesamt
time_interval = 8  # Zum Beispiel Intervalle von 8 Stunden

for start_time in tqdm(range(0, max_time, time_interval), desc='Aggregating SHAP values'):
    end_time = start_time + time_interval
    mask = (combined_labels[:, 0] >= start_time) & (combined_labels[:, 0] < end_time)
    aggregated_shap_values_A = np.abs(shap_values_A[mask]).mean(axis=0)
    aggregated_shap_values_B = np.abs(shap_values_B[mask]).mean(axis=0)
    aggregated_shap_values_list_A.append(aggregated_shap_values_A)
    aggregated_shap_values_list_B.append(aggregated_shap_values_B)

# Ausgeben der aggregierten SHAP-Werte für jedes Zeitintervall
feature_names_params = ["Cell Radius", "Area", "Migration Radius", "Distribution of Migration", "Number of Projections", "Circularity"]
for interval in range(len(aggregated_shap_values_list_A)):
    print(f"Zeitintervall {interval * time_interval} bis {(interval + 1) * time_interval} Stunden für Modell A:")
    for feature_idx, feature_name in enumerate(feature_names_params):
        print(f"{feature_name}: {aggregated_shap_values_list_A[interval][feature_idx]}")
    print("\nSHAP-Werte für Interaktionen im Modell B:")
    print(aggregated_shap_values_list_B[interval])
    print("\n")




# Aggregierte SHAP-Werte für Modell A und B in ein DataFrame umwandeln
aggregated_shap_values_df_A = pd.DataFrame(aggregated_shap_values_list_A, columns=feature_names_params)
aggregated_shap_values_df_B = pd.DataFrame(aggregated_shap_values_list_B)

# Ein DataFrame für die aggregierten SHAP-Werte erstellen
aggregated_shap_values_df = pd.concat([aggregated_shap_values_df_A, aggregated_shap_values_df_B], axis=1)

# Die Namen der Features für Modell B definieren (optional)
feature_names_cnn = [f"CNN_Feature_{i}" for i in range(1, aggregated_shap_values_df_B.shape[1] + 1)]
aggregated_shap_values_df_B.columns = feature_names_cnn

# CSV-Datei speichern
aggregated_shap_values_df.to_csv('/content/drive/My Drive/CNNMODEL/SHAP_ValuesInteraktion29.01.2024.csv', index=False)

print("Die SHAP-Werte wurden erfolgreich in Google Drive gespeichert.")

!pip install shap
import numpy as np
import shap
from sklearn.model_selection import train_test_split


# SHAP-Explainer für Modell A erstellen
explainer_A = shap.Explainer(model_A)

# SHAP-Werte für das Trainingsset berechnen (hier für X_train_params)
shap_values_A = explainer_A.shap_values(X_train_params)

# Feature-Namen für die Visualisierung festlegen
feature_names_params = ["Cell Radius", "Area", "Migration Radius", "Distribution of Migration", "Number of Projections", "Circularity"]

# Initialisieren der Liste, um die aggregierten SHAP-Werte zu speichern
aggregated_shap_values_list = []

# Maximalzeit und Zeitintervall definieren
max_time = 144  # Zum Beispiel 144 Stunden insgesamt
time_interval = 8  # Zum Beispiel Intervalle von 8 Stunden

# Berechnen der aggregierten SHAP-Werte für jedes Zeitintervall
for start_time in range(0, max_time, time_interval):
    end_time = start_time + time_interval
    # Maske, um Daten für das aktuelle Zeitintervall zu filtern
    mask = (combined_labels[:, 0] >= start_time) & (combined_labels[:, 0] < end_time)
    # Berechnung der durchschnittlichen absoluten SHAP-Werte für das aktuelle Intervall
    aggregated_shap_values = np.abs(shap_values_A[mask]).mean(axis=0)
    aggregated_shap_values_list.append(aggregated_shap_values)

# Konvertieren in ein NumPy-Array für die weitere Verarbeitung
aggregated_shap_values_A = shap_values_A[mask].mean(axis=0)

# Ausgeben der aggregierten SHAP-Werte für jedes Zeitintervall
for interval in range(len(aggregated_shap_values_array)):
    print(f"Zeitintervall {interval * time_interval} bis {(interval + 1) * time_interval} Stunden:")
    for feature_idx, feature_name in enumerate(feature_names_params):
        print(f"{feature_name}: {aggregated_shap_values_array[interval, feature_idx]}")
    print()  # Leerzeile für bessere Lesbarkeit

import pandas as pd
import numpy as np
import shap
from sklearn.model_selection import train_test_split

# Angenommen, explainer_A und X_train_params sind bereits definiert durchschnittliche WERTE
shap_values_A = explainer_A.shap_values(X_train_params)

# Feature-Namen und Zeitintervalle
feature_names_params = ["Cell Radius", "Area", "Migration Radius", "Distribution of Migration", "Number of Projections", "Circularity"]
max_time = 144
time_interval = 8

# Berechnen der durchschnittlichen SHAP-Werte
aggregated_shap_values_list = []
for start_time in range(0, max_time, time_interval):
    end_time = start_time + time_interval
    mask = (combined_labels[:, 0] >= start_time) & (combined_labels[:, 0] < end_time)
    if np.any(mask):
        # Durchschnittliche SHAP-Werte (nicht absolut) für das aktuelle Intervall
        mean_shap_values = shap_values_A[mask].mean(axis=0)
        aggregated_shap_values_list.append(mean_shap_values)
    else:
        # Fügen Sie eine Reihe von Nullen hinzu, wenn keine Daten für das Intervall vorhanden sind
        aggregated_shap_values_list.append(np.zeros(len(feature_names_params)))

# DataFrame erstellen
aggregated_shap_values_df = pd.DataFrame(aggregated_shap_values_list, columns=feature_names_params)

# Zeitintervalle als Index setzen
aggregated_shap_values_df.index = [f"{start_time}-{start_time + time_interval} Stunden" for start_time in range(0, max_time, time_interval)]

# CSV-Datei speichern
csv_path = '/content/drive/My Drive/CNNMODEL/ModelA-29.01.2024SHAP_Values_Mean.csv'
aggregated_shap_values_df.to_csv(csv_path)

print(f"Die durchschnittlichen SHAP-Werte wurden als CSV in {csv_path} gespeichert.")

import numpy as np
import shap
import tensorflow as tf
import pandas as pd
from tensorflow.keras.preprocessing.image import ImageDataGenerator



def load_and_extract_features(path_to_data, csv_file, images_folder, batch_size=18, selected_features=None):
    data = pd.read_csv(path_to_data + csv_file, sep=';')
    data['Image Name '] = data['Image Name '].apply(lambda x: f'{x}.tif')
    datagen = ImageDataGenerator(rescale=1./255)
    generator = datagen.flow_from_dataframe(
        dataframe=data,
        directory=path_to_data + images_folder,
        x_col="Image Name ",
        y_col=["Time", "Cell Radius", "Area", "Migration Radius", "Distrubition of Migration", "Number of Projections", "Compactness"],
        target_size=(299, 299),
        batch_size=batch_size,
        class_mode='raw'
    )

    original_images = []
    features = []
    labels = []
    for _ in range(len(generator)):
        x, y = generator.next()
        original_images.append(x)
        extracted_features = extractor.predict([x, x])
        if selected_features is not None:
            extracted_features = extracted_features[:, selected_features]
        features.append(extracted_features)
        labels.append(y)

    return np.concatenate(original_images, axis=0), np.concatenate(features, axis=0), np.concatenate(labels, axis=0)







# Angenommene Indizes Ihrer spezifischen Features
selected_feature_indices = [0, 96, 103, 153, 156, 162, 172, 222, 233, 251, 256, 275, 293, 314, 354, 357, 358, 366, 382, 391, 409, 505]

# Laden der ursprünglichen Bilder aus beiden Trainingsdatensätzen
original_images_1, extracted_features_1, labels_1 = load_and_extract_features(path_to_data, 'normalized_trainimages1.csv', 'train_images1tif', batch_size=18, selected_features=selected_feature_indices)
original_images_2, extracted_features_2, labels_2 = load_and_extract_features(path_to_data, 'normalized_trainimages2.csv', 'train_images2tif', batch_size=18, selected_features=selected_feature_indices)
original_images_3, extracted_features_3, labels_3 = load_and_extract_features(path_to_data, 'normalized_testimages.csv', 'test_imagestif', batch_size=18, selected_features=selected_feature_indices)


# Kombinieren der Bilder aus beiden Datensätzen
combined_original_images = np.concatenate([original_images_1, original_images_2, original_images_3], axis=0)

# Initialisieren des TreeExplainers für das XGBoost-Modell
explainer_B = shap.TreeExplainer(model_B)

# Verwenden Sie nur eine Teilmenge der extrahierten Features
subset_extracted_features = np.concatenate([extracted_features_1, extracted_features_2], axis=0)[:100]

# Berechnen der SHAP-Werte für die ausgewählten Features
shap_values_B = explainer_B.shap_values(subset_extracted_features)



# Anpassen der Labels auf die gleiche Teilmenge
subset_labels = np.concatenate([labels_1, labels_2], axis=0)[:100]

# Aggregierte SHAP-Werte berechnen
aggregated_shap_values_list_B = []

# Maximalzeit und Zeitintervall definieren
max_time = 144  # z.B. 144 Stunden insgesamt
time_interval = 8  # z.B. Intervalle von 8 Stunden

# Berechnen der aggregierten SHAP-Werte für jedes Zeitintervall
for start_time in range(0, max_time, time_interval):
    end_time = start_time + time_interval
    mask = (subset_labels[:, 0] >= start_time) & (subset_labels[:, 0] < end_time)
    aggregated_shap_values_B = np.abs(shap_values_B[mask]).mean(axis=0)
    aggregated_shap_values_list_B.append(aggregated_shap_values_B)

# Konvertieren in ein NumPy-Array für die weitere Verarbeitung
aggregated_shap_values_array_B = np.array(aggregated_shap_values_list_B)

# Ausgeben der aggregierten SHAP-Werte für jedes Zeitintervall
for interval in range(len(aggregated_shap_values_array_B)):
    print(f"Zeitintervall {interval * time_interval} bis {(interval + 1) * time_interval} Stunden:")
    for feature_idx, shap_value in enumerate(aggregated_shap_values_array_B[interval]):
        print(f"{feature_names_cnn[feature_idx]}: {shap_value}")
    print()  # Leerzeile für bessere Lesbarkeit



    # Aggregating average SHAP values over time intervals
average_shap_values_over_time = []

for start_time in range(0, max_time, time_interval):
    end_time = start_time + time_interval
    mask = (subset_labels[:, 0] >= start_time) & (subset_labels[:, 0] < end_time)

    if mask.any():
        average_shap_values = shap_values_B[mask].mean(axis=0)
    else:
        # If no data points in this interval, use a zero array
        average_shap_values = np.zeros(shap_values_B.shape[1])

    average_shap_values_over_time.append(average_shap_values)

# Convert to DataFrame for saving as CSV
df_average_shap_values = pd.DataFrame(average_shap_values_over_time, columns=[f'Feature_{idx}' for idx in range(shap_values_B.shape[1])])
csv_path = '/content/drive/My Drive/CNNMODEL/MODELBSHAP_Values_Average_Over_Time.csv'
df_average_shap_values.to_csv(csv_path, index=False)

print(f"Average SHAP values saved to {csv_path}")

import numpy as np
import shap
import tensorflow as tf
import pandas as pd
from tensorflow.keras.preprocessing.image import ImageDataGenerator



def load_and_extract_features(path_to_data, csv_file, images_folder, batch_size=18, selected_features=None):
    data = pd.read_csv(path_to_data + csv_file, sep=';')
    data['Image Name '] = data['Image Name '].apply(lambda x: f'{x}.tif')
    datagen = ImageDataGenerator(rescale=1./255)
    generator = datagen.flow_from_dataframe(
        dataframe=data,
        directory=path_to_data + images_folder,
        x_col="Image Name ",
        y_col=["Time", "Cell Radius", "Area", "Migration Radius", "Distrubition of Migration", "Number of Projections", "Compactness"],
        target_size=(299, 299),
        batch_size=batch_size,
        class_mode='raw'
    )

    original_images = []
    features = []
    labels = []
    for _ in range(len(generator)):
        x, y = generator.next()
        original_images.append(x)
        extracted_features = extractor.predict([x, x])
        if selected_features is not None:
            extracted_features = extracted_features[:, selected_features]
        features.append(extracted_features)
        labels.append(y)

    return np.concatenate(original_images, axis=0), np.concatenate(features, axis=0), np.concatenate(labels, axis=0)







# Angenommene Indizes Ihrer spezifischen Features
selected_feature_indices = [0, 96, 103, 153, 156, 162, 172, 222, 233, 251, 256, 275, 293, 314, 354, 357, 358, 366, 382, 391, 409, 505]

# Laden der ursprünglichen Bilder aus beiden Trainingsdatensätzen
original_images_1, extracted_features_1, labels_1 = load_and_extract_features(path_to_data, 'normalized_trainimages1.csv', 'train_images1tif', batch_size=18, selected_features=selected_feature_indices)
original_images_2, extracted_features_2, labels_2 = load_and_extract_features(path_to_data, 'normalized_trainimages2.csv', 'train_images2tif', batch_size=18, selected_features=selected_feature_indices)
original_images_3, extracted_features_3, labels_3 = load_and_extract_features(path_to_data, 'normalized_testimages.csv', 'test_imagestif', batch_size=18, selected_features=selected_feature_indices)


# Kombinieren der Bilder aus beiden Datensätzen
combined_original_images = np.concatenate([original_images_1, original_images_2, original_images_3], axis=0)

# Initialisieren des TreeExplainers für das XGBoost-Modell
explainer_B = shap.TreeExplainer(model_B)

# Verwenden Sie nur eine Teilmenge der extrahierten Features
subset_extracted_features = np.concatenate([extracted_features_1, extracted_features_2], axis=0)[:100]


print("Shape of the data used for SHAP: ", subset_extracted_features.shape)
print("Number of features in the trained model: ", len(model_B.get_booster().feature_names))



# Berechnen der SHAP-Werte für die ausgewählten Features
shap_values_B = explainer_B.shap_values(subset_extracted_features)





# Anpassen der Labels auf die gleiche Teilmenge
subset_labels = np.concatenate([labels_1, labels_2], axis=0)[:100]

# Aggregierte SHAP-Werte berechnen
aggregated_shap_values_list_B = []

# Maximalzeit und Zeitintervall definieren
max_time = 144  # z.B. 144 Stunden insgesamt
time_interval = 8  # z.B. Intervalle von 8 Stunden

# Berechnen der aggregierten SHAP-Werte für jedes Zeitintervall
for start_time in range(0, max_time, time_interval):
    end_time = start_time + time_interval
    mask = (subset_labels[:, 0] >= start_time) & (subset_labels[:, 0] < end_time)
    aggregated_shap_values_B = np.abs(shap_values_B[mask]).mean(axis=0)
    aggregated_shap_values_list_B.append(aggregated_shap_values_B)

# Konvertieren in ein NumPy-Array für die weitere Verarbeitung
aggregated_shap_values_array_B = np.array(aggregated_shap_values_list_B)

# Ausgeben der aggregierten SHAP-Werte für jedes Zeitintervall
for interval in range(len(aggregated_shap_values_array_B)):
    print(f"Zeitintervall {interval * time_interval} bis {(interval + 1) * time_interval} Stunden:")
    for feature_idx, shap_value in enumerate(aggregated_shap_values_array_B[interval]):
        print(f"{feature_names_cnn[feature_idx]}: {shap_value}")
    print()  # Leerzeile für bessere Lesbarkeit



# Hier ist Ihr Code, der die SHAP-Werte berechnet

# Nachdem Sie die SHAP-Werte berechnet haben, können Sie die durchschnittlichen Werte berechnen
average_shap_values = np.mean(shap_values_B, axis=0)

# Um die Ergebnisse als CSV zu speichern, verwenden Sie pandas
import pandas as pd

print("Shape of model input during training:", model_B._Booster.feature_names.shape)
print("Shape of data passed to SHAP explainer:", subset_extracted_features.shape)


# Erstellen Sie einen DataFrame mit den durchschnittlichen SHAP-Werten
df_average_shap = pd.DataFrame(average_shap_values, columns=['Average SHAP'])

# Speichern Sie den DataFrame als CSV in Google Drive
# Stellen Sie sicher, dass Sie den Pfad anpassen, wo Sie die Datei speichern möchten
df_average_shap.to_csv('/content/drive/My Drive/CNNMODEL/MODELBaverage_shap_values.csv', index=False)

import numpy as np
import shap
import tensorflow as tf
import pandas as pd
from tensorflow.keras.preprocessing.image import ImageDataGenerator

#sowohl postie als auch negative Einflus

def load_and_extract_features(path_to_data, csv_file, images_folder, batch_size=18, selected_features=None):
    data = pd.read_csv(path_to_data + csv_file, sep=';')
    data['Image Name '] = data['Image Name '].apply(lambda x: f'{x}.tif')
    datagen = ImageDataGenerator(rescale=1./255)
    generator = datagen.flow_from_dataframe(
        dataframe=data,
        directory=path_to_data + images_folder,
        x_col="Image Name ",
        y_col=["Time", "Cell Radius", "Area", "Migration Radius", "Distrubition of Migration", "Number of Projections", "Compactness"],
        target_size=(299, 299),
        batch_size=batch_size,
        class_mode='raw'
    )

    original_images = []
    features = []
    labels = []
    for _ in range(len(generator)):
        x, y = generator.next()
        original_images.append(x)
        extracted_features = extractor.predict([x, x])
        if selected_features is not None:
            extracted_features = extracted_features[:, selected_features]
        features.append(extracted_features)
        labels.append(y)

    return np.concatenate(original_images, axis=0), np.concatenate(features, axis=0), np.concatenate(labels, axis=0)







# Angenommene Indizes Ihrer spezifischen Features
selected_feature_indices = [0, 96, 103, 153, 156, 162, 172, 222, 233, 251, 256, 275, 293, 314, 354, 357, 358, 366, 382, 391, 409, 505]

# Laden der ursprünglichen Bilder aus beiden Trainingsdatensätzen
original_images_1, extracted_features_1, labels_1 = load_and_extract_features(path_to_data, 'normalized_trainimages1.csv', 'train_images1tif', batch_size=18, selected_features=selected_feature_indices)
original_images_2, extracted_features_2, labels_2 = load_and_extract_features(path_to_data, 'normalized_trainimages2.csv', 'train_images2tif', batch_size=18, selected_features=selected_feature_indices)
original_images_3, extracted_features_3, labels_3 = load_and_extract_features(path_to_data, 'normalized_testimages.csv', 'test_imagestif', batch_size=18, selected_features=selected_feature_indices)


# Kombinieren der Bilder aus beiden Datensätzen
combined_original_images = np.concatenate([original_images_1, original_images_2, original_images_3], axis=0)

# Initialisieren des TreeExplainers für das XGBoost-Modell
explainer_B = shap.TreeExplainer(model_B)

# Verwenden Sie nur eine Teilmenge der extrahierten Features
subset_extracted_features = np.concatenate([extracted_features_1, extracted_features_2], axis=0)[:100]

# Berechnen der SHAP-Werte für die ausgewählten Features
shap_values_B = explainer_B.shap_values(subset_extracted_features)



# Anpassen der Labels auf die gleiche Teilmenge
subset_labels = np.concatenate([labels_1, labels_2], axis=0)[:100]

# Aggregierte SHAP-Werte berechnen
aggregated_shap_values_list_B = []

# Maximalzeit und Zeitintervall definieren
max_time = 144  # z.B. 144 Stunden insgesamt
time_interval = 8  # z.B. Intervalle von 8 Stunden

# Berechnen der aggregierten SHAP-Werte für jedes Zeitintervall
for start_time in range(0, max_time, time_interval):
    end_time = start_time + time_interval
    mask = (subset_labels[:, 0] >= start_time) & (subset_labels[:, 0] < end_time)
    aggregated_shap_values_B = np.abs(shap_values_B[mask]).mean(axis=0)
    aggregated_shap_values_list_B.append(aggregated_shap_values_B)

# Konvertieren in ein NumPy-Array für die weitere Verarbeitung
aggregated_shap_values_B = shap_values_B[mask].mean(axis=0)

# Ausgeben der aggregierten SHAP-Werte für jedes Zeitintervall
for interval in range(len(aggregated_shap_values_array_B)):
    print(f"Zeitintervall {interval * time_interval} bis {(interval + 1) * time_interval} Stunden:")
    for feature_idx, shap_value in enumerate(aggregated_shap_values_array_B[interval]):
        print(f"{feature_names_cnn[feature_idx]}: {shap_value}")
    print()  # Leerzeile für bessere Lesbarkeit

# Überprüfung der Form der kombinierten Bilder
print("Form der kombinierten Bilder:", combined_original_images.shape)

# Wenn die Form nicht (None, 299, 299, 3) ist, müssen Sie die Bilder entsprechend anpassen

# Features extrahieren, indem zwei identische Eingaben an den Extractor übergeben werden
try:
    extracted_features = extractor.predict([combined_original_images, combined_original_images])
except Exception as e:
    print("Fehler beim Extrahieren der Features:", e)

# Überprüfen der Dimensionen der Originalbilder
print("Dimensionen von original_images_1:", original_images_1.shape)
print("Dimensionen von original_images_2:", original_images_2.shape)

# Basierend auf dem Output können wir feststellen, ob eine Anpassung notwendig ist

# Namen aller Schichten des Modells anzeigen
for layer in model.layers:
    print(layer.name)

# Feature-Extractor anpassen, um die Ausgabe der vorletzten Schicht zu extrahieren
extractor = tf.keras.Model(inputs=model.inputs, outputs=model.get_layer('batch_normalization_8').output)